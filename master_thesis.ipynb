{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d418fb7-3120-477d-abb4-3d5e4c8cac0c",
   "metadata": {},
   "source": [
    "# Domain Adaptation for Aerial Imagery using Generative Adversarial Networks for Semantic Segmentation\n",
    "\n",
    "Author: Takayuki Ota\n",
    "takayuki.ota@campus.tu-berlin.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed7971-657e-4805-960c-ead7db007695",
   "metadata": {},
   "source": [
    "# 1. Import libraries and prepare GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f173fd8-ed91-40fc-a8b7-fadb272ec3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct  5 18:08:22 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     Off  | 00000000:03:00.0 Off |                    0 |\n",
      "| 33%   24C    P8    15W / 260W |      0MiB / 22696MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 6000     Off  | 00000000:06:00.0 Off |                    0 |\n",
      "| 33%   27C    P8    23W / 260W |      0MiB / 22696MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 4000     Off  | 00000000:07:00.0 Off |                  N/A |\n",
      "| 30%   27C    P8     7W / 125W |      0MiB /  7982MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 4000     Off  | 00000000:83:00.0 Off |                  N/A |\n",
      "| 30%   26C    P8     7W / 125W |      0MiB /  7982MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Quadro RTX 4000     Off  | 00000000:86:00.0 Off |                  N/A |\n",
      "|  0%   24C    P8     1W / 125W |      0MiB /  7982MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Quadro RTX 5000     Off  | 00000000:87:00.0 Off |                  Off |\n",
      "|  0%   24C    P8     1W / 230W |      0MiB / 16125MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Collecting segmentation_models\n",
      "  Using cached segmentation_models-1.0.1-py3-none-any.whl (33 kB)\n",
      "Collecting efficientnet==1.0.0\n",
      "  Using cached efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
      "Collecting image-classifiers==1.0.0\n",
      "  Using cached image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting keras-applications<=1.0.8,>=1.0.7\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.9/site-packages (from efficientnet==1.0.0->segmentation_models) (0.19.2)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.9/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.9/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (21.3)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.19.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.9/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.8.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.9/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.2)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.9/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2021.11.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (9.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->scikit-image->efficientnet==1.0.0->segmentation_models) (3.0.9)\n",
      "\u001b[33mWARNING: Error parsing requirements for shapely: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/Shapely-1.8.2.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: keras-applications, image-classifiers, efficientnet, segmentation_models\n",
      "Successfully installed efficientnet-1.0.0 image-classifiers-1.0.0 keras-applications-1.0.8 segmentation_models-1.0.1\n",
      "Collecting patchify\n",
      "  Using cached patchify-0.2.3-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.9/site-packages (from patchify) (1.21.6)\n",
      "\u001b[33mWARNING: Error parsing requirements for shapely: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/Shapely-1.8.2.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: patchify\n",
      "Successfully installed patchify-0.2.3\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.9/site-packages (9.1.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for shapely: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/Shapely-1.8.2.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!pip install -U segmentation_models\n",
    "!pip install patchify\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f434fc0-e751-4926-abe2-0ebab6cfe9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from patchify import patchify\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "SM_FRAMEWORK=tf.keras\n",
    "import segmentation_models as sm\n",
    "sm.set_framework('tf.keras')\n",
    "\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d54240-3988-43af-b0a0-18c560b9aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n",
      "\n",
      "Available GPU Devices:\n",
      "  /physical_device:GPU:0 GPU\n",
      "  /physical_device:GPU:1 GPU\n",
      "  /physical_device:GPU:2 GPU\n",
      "  /physical_device:GPU:3 GPU\n",
      "  /physical_device:GPU:4 GPU\n",
      "  /physical_device:GPU:5 GPU\n",
      "\n",
      "Visible GPU Devices:\n",
      "  /physical_device:GPU:1 GPU\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = 1\n",
    "\n",
    "# Print the installed TensorFlow version\n",
    "print(f'TensorFlow version: {tf.__version__}\\n')\n",
    "\n",
    "# Get all GPU devices on this server\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all GPU devices\n",
    "print('Available GPU Devices:')\n",
    "for gpu in gpu_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set only the GPU specified as USE_GPU to be visible\n",
    "tf.config.set_visible_devices(gpu_devices[USE_GPU], 'GPU')\n",
    "\n",
    "# Get all visible GPU  devices on this server\n",
    "visible_devices = tf.config.get_visible_devices('GPU')\n",
    "\n",
    "# Print the name and the type of all visible GPU devices\n",
    "print('\\nVisible GPU Devices:')\n",
    "for gpu in visible_devices:\n",
    "    print(' ', gpu.name, gpu.device_type)\n",
    "    \n",
    "# Set the visible device(s) to not allocate all available memory at once,\n",
    "# but rather let the memory grow whenever needed\n",
    "for gpu in visible_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375618d7-bf75-4e90-9ff1-cac5c1576a40",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Preprocess images.  ※Skip to section 3 if you already have patched, ready-to-analyze data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc44b6-a696-45e8-b67e-3f6314ec39d1",
   "metadata": {},
   "source": [
    "## 2.1 Split each image into patches\n",
    "\n",
    "We used Inria Aerial Imagery Lebeling Dataset(https://project.inria.fr/aerialimagelabeling/)\n",
    "\n",
    "A few sample images and labels are available at our github repository for quick test run.\n",
    "\n",
    "The original dataset consists of 36 images and labels for each domain and there are five domains(Austin, Chicago, Kitsap, Tyrol, Vienna).\n",
    "\n",
    "Here are some features of the dataset:\n",
    "\n",
    "- Coverage of 810 km² (405 km² for training and 405 km² for testing)\n",
    "- Aerial orthorectified color imagery with a spatial resolution of 0.3 m\n",
    "- Ground truth data for two semantic classes: building and not building \n",
    "\n",
    "\n",
    "- 36 images of 5000x5000 pixel per domain\n",
    "- Crop to nearest divisible by patch size(256) -> 4864x4864\n",
    "- 4864/256 = 19 \n",
    "- 19 * 19 = 361 patches per image\n",
    "- 361 * 36 = 12996 images in total in one domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35517b0e-0134-4646-b265-b8c19dbb4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose patch size\n",
    "PATCH_SIZE = 256\n",
    "image_directory = 'master_thesis/dataset/Inria/austin/images'\n",
    "patched_image_directory = \"master_thesis/dataset/Inria/austin/patched_images\"\n",
    "\n",
    "label_directory = \"master_thesis/dataset/Inria/austin/labels\"\n",
    "patched_label_directory = \"master_thesis/dataset/Inria/austin/patched_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96977433-3bf5-4f2f-8884-8fb21c81e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patch(large_image, PATCH_SIZE):\n",
    "    SIZE_X = (large_image.shape[1] // PATCH_SIZE) * PATCH_SIZE  # Nearest size divisible by our patch size(5888)\n",
    "    SIZE_Y = (large_image.shape[0] // PATCH_SIZE) * PATCH_SIZE  # Nearest size divisible by our patch size(5888)\n",
    "    image = Image.fromarray(large_image)\n",
    "    image = image.crop((0, 0, SIZE_X, SIZE_Y))  # Crop from top left corner\n",
    "    image = np.array(image)\n",
    "    print(\"Now patchifying image:\",)\n",
    "    patched_image = patchify(image, (PATCH_SIZE, PATCH_SIZE, 3),step=PATCH_SIZE)  # Step=256 for 256 patches means no overlap. 23 patches of 256x256\n",
    "    \n",
    "    return patched_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135e305-8480-4b03-95a1-18a0d9a497d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split images into patches\n",
    "# Uncomment cv2.imwrite if you want to save the patched images.\n",
    "image_dataset = []\n",
    "for path in sorted(os.listdir(image_directory)):    \n",
    "    image = cv2.imread(image_directory + \"/\" + path, 1) # BGR, not RGB!\n",
    "    if path.endswith(\".png\"):\n",
    "        patched_image = make_patch(image, PATCH_SIZE)\n",
    "        for i in range(patched_image.shape[0]):\n",
    "            for j in range(patched_image.shape[1]):\n",
    "                    single_patched_image = patched_image[i, j, :, :]\n",
    "                    single_patched_image = single_patched_image[0]  # Drop the extra unecessary dimension that patchify adds.\n",
    "                    # cv2.imwrite(\"master_thesis/dataset/Inria/austin/patched_images/\"+ path +str(i)+ \"_\" + str(j)+\".png\", single_patched_image)\n",
    "                    print(\"patch_\"+ path + str(i)+ \"_\" + str(j)+\".png\")\n",
    "                    image_dataset.append(single_patched_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d23e9-aec0-4d6b-a169-28548101a4be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the same processing to the label\n",
    "# Split images into patches\n",
    "# Uncomment cv2.imwrite if you want to save the patched images.\n",
    "label_dataset = []\n",
    "for path in sorted(os.listdir(label_directory)):    \n",
    "    image = cv2.imread(label_directory + \"/\" + path, 1) # BGR, not RGB!\n",
    "    if path.endswith(\".png\"):\n",
    "        patched_image = make_patch(image, PATCH_SIZE)\n",
    "        for i in range(patched_image.shape[0]):\n",
    "            for j in range(patched_image.shape[1]):\n",
    "                single_patched_image = patched_image[i, j, :, :]\n",
    "                single_patched_image = single_patched_image[0]  # Drop the extra unecessary dimension that patchify adds.\n",
    "                # cv2.imwrite(\"master_thesis/dataset/Inria/austin/patched_labels/\"+ path +str(i)+ \"_\" + str(j)+\".png\", single_patched_image)\n",
    "                print(\"patch_label_\"+ path +str(i)+ \"_\" + str(j)+\".png\")\n",
    "                label_dataset.append(single_patched_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201212aa-1519-4bd0-97c0-92e5b2169d1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2 Split folders into train/test/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610b2d9-a903-4154-baaa-ac4939838b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cd579-31d9-44bb-ac74-09475edd3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders  \n",
    "input_folder = 'master_thesis/dataset/Inria/austin/patched_images/'\n",
    "output_folder='master_thesis/dataset/Inria/austin/patched_pipeline/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905bf8e3-f860-4a7f-9683-0d4bc651f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with a ratio.\n",
    "# Training/test/validation = 8/1/1\n",
    "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.8, .1, .1), group_prefix=None) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9edc273-eff7-4e4b-972a-4bf79a908f4d",
   "metadata": {},
   "source": [
    "Code for splitting folder into train, test, and val. Once the new folders are created, rename them and arrange in the format below to be used for semantic segmentation using data generators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812b7c1-7466-4b0a-93f2-9c8d932abc1f",
   "metadata": {},
   "source": [
    "\n",
    "For semantic segmentation the folder structure needs to look like below\n",
    "if you want to use ImageDatagenerator.\n",
    "So after splitting your folders to train, val and possibly also test, rearrange them to the following format. \n",
    "\n",
    "Data/\n",
    "\n",
    "    train_images/\n",
    "                train/\n",
    "                    img1, img2, img3, ......\n",
    "    \n",
    "    train_masks/\n",
    "                train/\n",
    "                    msk1, msk, msk3, ......\n",
    "                    \n",
    "    val_images/\n",
    "                val/\n",
    "                    img1, img2, img3, ......                \n",
    "\n",
    "    val_masks/\n",
    "                val/\n",
    "                    msk1, msk, msk3, ......\n",
    "      \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35724bb5-4773-46f3-90b7-911229a6fbf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Build data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc631e81-6684-43a2-957d-04ba475a1ea2",
   "metadata": {},
   "source": [
    "## 3.1 Define Generator for images and masks so we can read them directly from the drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0c153-43cd-4705-98f7-69aae3d36eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=123\n",
    "batch_size= 16\n",
    "n_classes=1\n",
    "\n",
    "#Define a function to perform additional preprocessing after datagen.\n",
    "#For example, scale images, convert masks to categorical, etc. \n",
    "def preprocess_data(img, mask, num_class):\n",
    "    #Scale images\n",
    "    img = scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)\n",
    "    mask= scaler.fit_transform(mask.reshape(-1, mask.shape[-1])).reshape(mask.shape)      \n",
    "    \n",
    "    return (img,mask)\n",
    "\n",
    "#Define the generator.\n",
    "#We are not doing any rotation or zoom to make sure mask values are not interpolated.\n",
    "#It is important to keep pixel values in mask.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def trainGenerator(train_img_path, train_mask_path, num_class):\n",
    "    \n",
    "    img_data_gen_args = dict(horizontal_flip=True,\n",
    "                      vertical_flip=True,\n",
    "                      fill_mode='reflect')\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_img_path,\n",
    "        class_mode = None,\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_mask_path,\n",
    "        class_mode = None,\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "    \n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    for (img, mask) in train_generator:\n",
    "        img, mask = preprocess_data(img, mask, num_class)\n",
    "        yield (img, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db0ce5a-2904-4c0a-b198-95002ed069b2",
   "metadata": {},
   "source": [
    "## 3.2 Declare your path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689228e-547f-4d64-8c81-4dd04ff1e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"master_thesis/dataset/Inria/austin/patched_pipeline/\")\n",
    "domain = Path(\"master_thesis/models/Inria/austin/\")\n",
    "model_name = Path(\"austin-Unet-50epochs-diceloss-batch16.hdf5\")\n",
    "\n",
    "train_img_path = data_path / \"train_images\"\n",
    "train_mask_path = data_path / \"train_masks\"\n",
    "\n",
    "val_img_path = data_path / \"val_images\"\n",
    "val_mask_path = data_path / \"val_masks\"\n",
    "\n",
    "train_img_gen = trainGenerator(train_img_path, train_mask_path, num_class=1)\n",
    "val_img_gen = trainGenerator(val_img_path, val_mask_path, num_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf5f82-5917-40ba-8b3a-783041403d74",
   "metadata": {},
   "source": [
    "## 3.3 Sanity check for images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9ea80-a28f-42ac-aa3c-352bcf76457c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Make sure the generator is working and that images and masks are indeed lined up. \n",
    "x, y = train_img_gen.__next__()\n",
    "\n",
    "for i in range(0,3):\n",
    "    image = x[i,:,:,]\n",
    "    mask = y[i,:,:,]\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(mask)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a230d9d-69c4-4ae6-a4be-dacda2f204ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply the same sanity check for validation data\n",
    "x_val, y_val = val_img_gen.__next__()\n",
    "\n",
    "for i in range(0,3):\n",
    "    image = x_val[i,:,:,]\n",
    "    mask = y_val[i,:,:,]\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(mask)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732f14d-fdd8-4669-98d4-be25bd540bc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# 4. Train segmentation models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124a85d-8fff-4713-90f2-319a5a8ca8c3",
   "metadata": {},
   "source": [
    "## 4.1 Define the model pamaters\n",
    "- model: U-Net\n",
    "- loss: Dice Loss\n",
    "- optimizer: Adam\n",
    "- metrics: Intersection Over Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841b1b6-485f-41f6-987e-e14433778f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_train_imgs = len(os.listdir(train_img_path/\"train/\"))\n",
    "num_val_images = len(os.listdir(val_img_path/\"val\"))\n",
    "\n",
    "steps_per_epoch = num_train_imgs//batch_size\n",
    "val_steps_per_epoch = num_val_images//batch_size\n",
    "\n",
    "IMG_HEIGHT = x.shape[1]\n",
    "IMG_WIDTH  = x.shape[2]\n",
    "IMG_CHANNELS = x.shape[3]\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "n_classes=1\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d000c-e5b2-4096-b3a0-3cff2e6baa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Unet(classes=1, activation='sigmoid',input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
    "\n",
    "dice_loss = sm.losses.DiceLoss()\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "\n",
    "# compile keras model with defined optimozer, loss and metrics\n",
    "model.compile(optimizer='adam',loss=dice_loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667d9b9-94fa-4b71-a40d-ba36f74b7220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4be7ae-21de-4950-8013-6f103240cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss',restore_best_weights=True),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='run_logdir'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(domain / \"checkpoint\" / model_name, verbose=1, save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e568d-9f54-4b8e-a96f-b6bfa9d108b4",
   "metadata": {},
   "source": [
    "## 4.2 Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5539a81-3b7c-497a-9501-15db58a68933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history=model.fit(train_img_gen,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=2,\n",
    "          verbose=1,\n",
    "          validation_data=val_img_gen,\n",
    "          validation_steps=val_steps_per_epoch,\n",
    "                 callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125650c9-95ea-4302-9e37-85f4ddc7149b",
   "metadata": {},
   "source": [
    "## 4.3 Plot the loss and IoU curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4297e-fbfd-416e-aa83-c39decd5c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# plt.savefig('master_thesis/training_logs/UNet-chicago-50epochs-16batches-diceloss-loss.png')\n",
    "plt.show()\n",
    "\n",
    "iou = history.history['iou_score']\n",
    "val_iou = history.history['val_iou_score']\n",
    "plt.plot(epochs, iou, 'y', label='Training IoU')\n",
    "plt.plot(epochs, val_iou, 'r', label='Validation IoU')\n",
    "plt.title('Training and validation IoU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend()\n",
    "# plt.savefig('master_thesis/training_logs/UNet-chicago-50epochs-16batches-diceloss-iou.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e34d8-9ecd-4e07-8dcb-70458720d441",
   "metadata": {},
   "source": [
    "## 4.4 View a few images, masks, and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe68a4f-f75a-412d-80cb-db1827a88249",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_batch, test_mask_batch = val_img_gen.__next__()\n",
    "img_num = random.randint(0, test_image_batch.shape[0]-1)\n",
    "\n",
    "prediction = (model.predict(test_image_batch)[img_num] > 0.5).astype(np.uint8)\n",
    "prediction = prediction[:,:,0]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_image_batch[img_num])\n",
    "plt.subplot(232)\n",
    "plt.title('Ground Truth')\n",
    "plt.imshow(test_mask_batch[img_num],cmap='gray')\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction')\n",
    "plt.imshow(prediction,cmap='gray')\n",
    "# plt.savefig('master_thesis/predictions/chicago/UNet-chicago-50epochs-16batches-diceloss-prediction-9.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1a7dd-9dbe-47b5-afe0-3a3bb7ddcb05",
   "metadata": {},
   "source": [
    "## 4.5 Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414abff-0bc6-4ffd-a2ad-42fe2efed5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(domain / model_name)\n",
    "# model.save('master_thesis/models/Inria/alldomain/Unet-50epochs-diceloss-batch16-sm.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d5fe6-e743-423b-9363-e9cd887fed73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e710c-f96c-453f-a733-b14451a7c13a",
   "metadata": {},
   "source": [
    "## 5.1 Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2218430d-3cfa-4aa7-9636-1b63eb67d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import segmentation_models as sm\n",
    "sm.set_framework('tf.keras')\n",
    "\n",
    "# restored_model = model\n",
    "restored_model = load_model(\"master_thesis/models/Inria/austin/viennastyled-Unet-50epochs-diceloss-batch16.hdf5\", compile=False)\n",
    "# restored_naive_model = load_model(\"master_thesis/models/Inria/austin/Unet-50epochs-diceloss-batch16-sm.hdf5\", compile=False)\n",
    "# restored_model_chicago = load_model(\"master_thesis/models/Inria/austin/chicagostyled-Unet-50epochs-diceloss-batch16.hdf5\", compile=False)\n",
    "# restored_model_kitsap = load_model(\"master_thesis/models/Inria/vienna/kitsapstyled-Unet-50epochs-diceloss-batch16.hdf5\", compile=False)\n",
    "# restored_model_tyrol = load_model(\"master_thesis/models/Inria/vienna/tyrolstyled-Unet-50epochs-diceloss-batch16.hdf5\", compile=False)\n",
    "# restored_model_vienna = load_model(\"master_thesis/models/Inria/vienna/viennastyled-Unet-50epochs-diceloss-batch16.hdf5\", compile=False)\n",
    "\n",
    "dice_loss = sm.losses.DiceLoss()\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "\n",
    "restored_model.compile(optimizer='adam',loss=dice_loss, metrics=metrics)\n",
    "# restored_naive_model.compile(optimizer='adam',loss=dice_loss, metrics=metrics)\n",
    "# restored_model_chicago.compile(optimizer='adam',loss=dice_loss, metrics=metrics)\n",
    "# restored_model_kitsap.compile(optimizer='adam',loss=dice_loss, metrics=metrics)\n",
    "# restored_model_tyrol.compile(optimizer='adam',loss=dice_loss, metrics=metrics)\n",
    "# restored_model_vienna.compile(optimizer='adam',loss=dice_loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d7d73-07e8-4f2e-933b-07779f0f3898",
   "metadata": {},
   "source": [
    "## 5.2 Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a2f83-4c69-408a-95b7-9fc1cc691b47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = restored_model.evaluate(val_img_gen, steps=100)\n",
    "# results_naive = restored_naive_model.evaluate(val_img_gen, steps=100)\n",
    "# results_chicago = restored_model_chicago.evaluate(val_img_gen, steps=100)\n",
    "# results_kitsap = restored_model_kitsap.evaluate(val_img_gen, steps=100)\n",
    "# results_tyrol = restored_model_tyrol.evaluate(val_img_gen, steps=100)\n",
    "# results_vienna = restored_model_vienna.evaluate(val_img_gen, steps=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c5e39-1ecb-4099-8681-30547318371b",
   "metadata": {},
   "source": [
    "# 6. Train CycleGAN to translate the style of image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb1cba-9b11-4eac-9ec9-be3a2b0abbb3",
   "metadata": {},
   "source": [
    "Implementation of this part is based on the officidal code repository of the O'Reilly book 'Generative Deep Learning'\n",
    "https://github.com/davidADSP/GDL_code/tree/tensorflow_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a4ece96-f026-4301-9e24-b528f7894b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Using cached tensorflow_addons-0.18.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting typeguard>=2.7\n",
      "  Using cached typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow-addons) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->tensorflow-addons) (3.0.9)\n",
      "\u001b[33mWARNING: Error parsing requirements for shapely: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/Shapely-1.8.2.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.18.0 typeguard-2.13.3\n",
      "Collecting pydot\n",
      "  Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.9/site-packages (from pydot) (3.0.9)\n",
      "\u001b[33mWARNING: Error parsing requirements for shapely: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/Shapely-1.8.2.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pydot\n",
      "Successfully installed pydot-1.4.2\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.9/site-packages (0.19.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for shapely: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/Shapely-1.8.2.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons\n",
    "!pip install pydot\n",
    "!pip install graphviz\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db4dfc43-4c4f-47ff-8fb5-eed2026acb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cycleGAN import CycleGAN\n",
    "from utils.loaders import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb2bf0-ffa8-405b-b57f-1ab15c3d5698",
   "metadata": {},
   "source": [
    "## 6.1 Set up parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42480c69-3d97-468f-9cf1-593371033fd0",
   "metadata": {},
   "source": [
    "Folder structure needs to be like below:\n",
    "\n",
    "data/\n",
    "     \n",
    "     austin2chicago/\n",
    "    \n",
    "        trainA/\n",
    "            img1, img2, img3 ...\n",
    "        testA/\n",
    "            img11, img12, img13 ...\n",
    "        trainB/\n",
    "            img1, img2, img3 ...\n",
    "        testB/\n",
    "            img11, img12, img13 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de236626-4b50-4493-bb22-74ef255b862c",
   "metadata": {},
   "source": [
    "For example, you can prepare training data for austin2chicago translation with this command.\n",
    "\n",
    "cp -a dataset/Inria/austin/patched_pipeline/train_images/train/. GDL_code/data/austin2chicago/trainA\n",
    "\n",
    "cp -a dataset/Inria/chicago/patched_pipeline/train_images/train/. GDL_code/data/austin2chicago/trainB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e0675b-a620-4eff-a054-945e0a0ec0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION = 'cyclegan'\n",
    "RUN_ID = '0005'\n",
    "DATA_NAME = 'austin2chicago'\n",
    "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "# choose each one test image for domain A and B. It will be used for plotting the progress of training\n",
    "TEST_A_FILE = \"austin1.png0_0.png\"\n",
    "TEST_B_FILE = \"chicago1.png0_1.png\"\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 1 #default = 1\n",
    "EPOCHS = 15 # default = 200\n",
    "PRINT_EVERY_N_BATCHES = 1000\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcdc534f-57fd-427e-86f3-7efbf133dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset_name=DATA_NAME, img_res=(IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161b615-160d-4bd1-be54-356b576eced7",
   "metadata": {},
   "source": [
    "## 6.2 cycleGAN architecture\n",
    "validation, reconstr, id are the parameters for three loss functions. Play around with them. Defalult is validation=1,reconstr=10,id=2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c07474-1acc-4856-bf59-011178887817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 18:09:14.409733: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-05 18:09:15.162045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21322 MB memory:  -> device: 1, name: Quadro RTX 6000, pci bus id: 0000:06:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "gan = CycleGAN(\n",
    "    input_dim = (IMAGE_SIZE,IMAGE_SIZE,3)\n",
    "    ,learning_rate = 0.0002\n",
    "    , buffer_max_length = 50\n",
    "    , lambda_validation = 1\n",
    "    , lambda_reconstr = 10\n",
    "    , lambda_id = 2\n",
    "    , generator_type = 'unet'\n",
    "    , gen_n_filters = 32\n",
    "    , disc_n_filters = 32\n",
    "    )\n",
    "if mode == 'build':\n",
    "    gan.save(RUN_FOLDER)\n",
    "else:\n",
    "    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))\n",
    "    print(\"Model\" + RUN_FOLDER+ \" is loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c308a-8942-4cd9-99c4-421cfade742d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gan.g_AB.summary()\n",
    "# gan.g_BA.summary()\n",
    "# gan.d_A.summary()\n",
    "# gan.d_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a4b556-5d28-4c2b-bb2c-7a6b965413cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 18:09:24.878717: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7f3574017f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[Epoch 0/15] [Batch 0/10396] [D loss: 2.276120, acc:  46%] [G loss: 17.572247, adv: 3.927239, recon: 1.163519, id: 1.004907] time: 0:00:12.815754 \n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "[Epoch 0/15] [Batch 1/10396] [D loss: 1.729079, acc:  45%] [G loss: 14.728640, adv: 2.784103, recon: 1.013024, id: 0.907149] time: 0:00:15.274809 \n",
      "[Epoch 0/15] [Batch 2/10396] [D loss: 1.104031, acc:  46%] [G loss: 13.579031, adv: 1.882098, recon: 0.992832, id: 0.884306] time: 0:00:15.522478 \n",
      "[Epoch 0/15] [Batch 3/10396] [D loss: 0.842696, acc:  48%] [G loss: 12.684430, adv: 1.496287, recon: 0.945545, id: 0.866346] time: 0:00:15.766270 \n",
      "[Epoch 0/15] [Batch 4/10396] [D loss: 0.714531, acc:  48%] [G loss: 12.123199, adv: 1.458541, recon: 0.894904, id: 0.857810] time: 0:00:16.007308 \n",
      "[Epoch 0/15] [Batch 5/10396] [D loss: 0.684619, acc:  48%] [G loss: 11.670338, adv: 1.725764, recon: 0.836237, id: 0.791103] time: 0:00:16.244539 \n",
      "[Epoch 0/15] [Batch 6/10396] [D loss: 0.740967, acc:  48%] [G loss: 11.075575, adv: 1.395129, recon: 0.817822, id: 0.751111] time: 0:00:16.482740 \n",
      "[Epoch 0/15] [Batch 7/10396] [D loss: 0.633125, acc:  47%] [G loss: 10.705682, adv: 1.315987, recon: 0.789185, id: 0.748924] time: 0:00:16.732296 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRUN_FOLDER\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_A_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTEST_A_FILE\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_B_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTEST_B_FILE\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPRINT_EVERY_N_BATCHES\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_thesis/GDL_code/models/cycleGAN.py:318\u001b[0m, in \u001b[0;36mCycleGAN.train\u001b[0;34m(self, data_loader, run_folder, epochs, test_A_file, test_B_file, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch, epochs):\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_i, (imgs_A, imgs_B) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader\u001b[38;5;241m.\u001b[39mload_batch()):\n\u001b[0;32m--> 318\u001b[0m         d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_discriminators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m         g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_generators(imgs_A, imgs_B, valid)\n\u001b[1;32m    321\u001b[0m         elapsed_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/master_thesis/GDL_code/models/cycleGAN.py:267\u001b[0m, in \u001b[0;36mCycleGAN.train_discriminators\u001b[0;34m(self, imgs_A, imgs_B, valid, fake)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_discriminators\u001b[39m(\u001b[38;5;28mself\u001b[39m, imgs_A, imgs_B, valid, fake):\n\u001b[1;32m    265\u001b[0m \n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# Translate images to opposite domain\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     fake_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg_AB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_A\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     fake_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_BA\u001b[38;5;241m.\u001b[39mpredict(imgs_B)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_B\u001b[38;5;241m.\u001b[39mappend(fake_B)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1951\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1944\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1945\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1946\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1947\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTPUStrategy and AutoShardPolicy.FILE might lead to out-of-order \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1948\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1949\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m-> 1951\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1954\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1399\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1149\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1148\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:236\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    226\u001b[0m              x,\n\u001b[1;32m    227\u001b[0m              y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m              shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    234\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    235\u001b[0m   \u001b[38;5;28msuper\u001b[39m(TensorLikeDataAdapter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 236\u001b[0m   x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_process_tensorlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m   sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    238\u001b[0m       sample_weights, sample_weight_modes)\n\u001b[1;32m    240\u001b[0m   \u001b[38;5;66;03m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1043\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[1;32m   1041\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m-> 1043\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_single_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1038\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_single_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[1;32m   1037\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[0;32m-> 1038\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[1;32m   1040\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1559\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m   1498\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1499\u001b[0m   \u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \n\u001b[1;32m   1501\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1559\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1565\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1564\u001b[0m   \u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1565\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m      \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1695\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1691\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1692\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1695\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1698\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     47\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = gan.train(data_loader\n",
    "        , run_folder = RUN_FOLDER\n",
    "        , epochs=EPOCHS\n",
    "        , test_A_file = TEST_A_FILE\n",
    "        , test_B_file = TEST_B_FILE\n",
    "        , batch_size=BATCH_SIZE\n",
    "        , sample_interval=PRINT_EVERY_N_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c458f3-9f80-4bd7-9129-0a4719b345a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot([x[1] for x in gan.g_losses], color='green', linewidth=0.5) #DISCRIM LOSS\n",
    "# plt.plot([x[2] for x in gan.g_losses], color='orange', linewidth=0.1)\n",
    "plt.plot([x[3] for x in gan.g_losses], color='blue', linewidth=0.5) #CYCLE LOSS\n",
    "# plt.plot([x[4] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "plt.plot([x[5] for x in gan.g_losses], color='red', linewidth=0.5) #ID LOSS\n",
    "# plt.plot([x[6] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.plot([x[0] for x in gan.g_losses], color='black', linewidth=0.25)\n",
    "\n",
    "# plt.plot([x[0] for x in gan.d_losses], color='black', linewidth=1)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "# plt.ylim(0, 5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6dc1b-32a0-481f-9fed-7b24bdcf2bd2",
   "metadata": {},
   "source": [
    "## 6.3 Translate the images using trained cycleGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb14d681-89fe-4b4e-a522-c5caf641fe90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2373/10396 [02:32<08:34, 15.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(source_images_dir\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m path, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m fake_B \u001b[38;5;241m=\u001b[39m \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg_AB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[1;32m     15\u001b[0m fake_B \u001b[38;5;241m=\u001b[39m fake_B[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Drop the extra unecessary dimension\u001b[39;00m\n\u001b[1;32m     16\u001b[0m fake_B \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(((fake_B\u001b[38;5;241m-\u001b[39mfake_B\u001b[38;5;241m.\u001b[39mmin())\u001b[38;5;241m/\u001b[39m(fake_B\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m-\u001b[39mfake_B\u001b[38;5;241m.\u001b[39mmin())\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# reverse the normalized data(-1~+1) to 0-255\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1978\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1976\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[1;32m   1977\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1978\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[1;32m   1979\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   1980\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/data_adapter.py:1191\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1191\u001b[0m   data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:486\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    485\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    489\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:755\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    751\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    754\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 755\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:787\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deleter \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    784\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v2(\n\u001b[1;32m    785\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[1;32m    786\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[0;32m--> 787\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m   \u001b[38;5;66;03m# Delete the resource when this object is deleted\u001b[39;00m\n\u001b[1;32m    789\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_deleter \u001b[38;5;241m=\u001b[39m IteratorResourceDeleter(\n\u001b[1;32m    790\u001b[0m       handle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource,\n\u001b[1;32m    791\u001b[0m       deleter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deleter)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3315\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3314\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3315\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3318\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "\n",
    "source_images_dir = \"data/austin2chicago/trainA\"\n",
    "translated_images_dir = \"data/austin2chicago/fakeB\"\n",
    "image_dataset = []\n",
    "fake_dataset = []\n",
    "\n",
    "for path in tqdm.tqdm(sorted(os.listdir(source_images_dir))):    \n",
    "    if path.endswith(\".png\"):\n",
    "        # print(path)\n",
    "        image = cv2.imread(source_images_dir+ \"/\" + path, 1)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        fake_B = gan.g_AB.predict(image)     \n",
    "        fake_B = fake_B[0]  # Drop the extra unecessary dimension\n",
    "        fake_B = Image.fromarray(((fake_B-fake_B.min())/(fake_B.max()-fake_B.min())*255).astype(np.uint8), 'RGB') # reverse the normalized data(-1~+1) to 0-255\n",
    "        fake_B.save(translated_images_dir + \"/fake_\" + path)\n",
    "        fake_dataset.append(fake_B)\n",
    "        image_dataset.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd0b202-a101-49d9-9d78-534340d15bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also translate from domain B to A\n",
    "source_images_dir = \"data/austin2chicago/trainB\"\n",
    "translated_images_dir = \"data/austin2chicago/fakeA\"\n",
    "image_dataset = []\n",
    "fake_dataset = []\n",
    "for path in tqdm.tqdm(sorted(os.listdir(source_images_dir))):    \n",
    "    if path.endswith(\".png\"):\n",
    "        # print(path)\n",
    "        image = cv2.imread(source_images_dir+ \"/\" + path, 1)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        fake_A = gan.g_BA.predict(image)\n",
    "        fake_A = fake_A[0]  # Drop the extra unecessary dimension\n",
    "        fake_A = Image.fromarray(((fake_A-fake_A.min())/(fake_A.max()-fake_A.min())*255).astype(np.uint8), 'RGB') # reverse the normalized data(-1~+1) to 0-255\n",
    "        fake_A.save(translated_images_dir + \"/fake_\" + path)\n",
    "        fake_dataset.append(fake_A)\n",
    "        image_dataset.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5872002-47c0-45ba-9ded-62d7e4f2e754",
   "metadata": {},
   "source": [
    "## 5.4 Go to step 3. and train the model using translated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0394d1-663c-445c-8fc0-6e92714a5369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
